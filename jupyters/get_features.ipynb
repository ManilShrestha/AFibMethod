{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (17235, 3750)\n",
      "Train Labels Shape: (17235,)\n",
      "Validation Features Shape: (1915, 3750)\n",
      "Validation Labels Shape: (1915,)\n",
      "Test Features Shape: (8207, 3750)\n",
      "Test Labels Shape: (8207,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to load data and split into features and labels\n",
    "def load_data(filepath):\n",
    "\t# Load the data from CSV file\n",
    "\tdata = np.genfromtxt(filepath, delimiter=',', skip_header=1)  # skip_header is used if your CSV has a header row\n",
    "\t# Separate features and labels\n",
    "\tX = data[:, :-1]  # all rows, all columns except the last\n",
    "\ty = data[:, -1]   # all rows, last column\n",
    "\treturn X, y\n",
    "\n",
    "# Load datasets\n",
    "X_train, y_train = load_data('../data/ABP_train_samples.csv')\n",
    "X_val, y_val = load_data('../data/ABP_val_samples.csv')\n",
    "X_test, y_test = load_data('../data/ABP_test_samples.csv')\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Train Features Shape:\", X_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Validation Features Shape:\", X_val.shape)\n",
    "print(\"Validation Labels Shape:\", y_val.shape)\n",
    "print(\"Test Features Shape:\", X_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing values from X_train that are less than or equal to 0 for more than 50% of duration\n",
    "mask = X_train <= 0\n",
    "count_le_zero = np.sum(mask, axis=1)\n",
    "# Find the rows where more than 50% of the elements are <= 0\n",
    "rows_to_remove = count_le_zero > (X_train.shape[1] / 2)\n",
    "\n",
    "rows_to_keep = ~rows_to_remove\n",
    "\n",
    "# Apply the mask to filter out the rows in X_train\n",
    "X_train_filtered = X_train[rows_to_keep]\n",
    "# Assuming y_train is a corresponding array of labels\n",
    "y_train_filtered = y_train[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "import scipy.signal as signal\n",
    "\n",
    "def compute_time_domain_statistics(data):\n",
    "\t# Calculate mean\n",
    "\tmean = np.mean(data, axis=1)\n",
    "\t\n",
    "\t# Calculate median\n",
    "\tmedian = np.median(data, axis=1)\n",
    "\t\n",
    "\t# Calculate standard deviation\n",
    "\tstd_dev = np.std(data, axis=1)\n",
    "\t\n",
    "\t# Calculate variance\n",
    "\tvariance = np.var(data, axis=1)\n",
    "\t\n",
    "\t# Calculate interquartile range\n",
    "\tiqr = stats.iqr(data, axis=1)\n",
    "\t\n",
    "\t# Calculate skewness\n",
    "\tskewness = stats.skew(data, axis=1)\n",
    "\t\n",
    "\t# Calculate kurtosis\n",
    "\tscaled_data = data * 1e6  # Scale up the data to avoid precision issues\n",
    "\tkurtosis = stats.kurtosis(scaled_data, axis=1)\n",
    "\t\n",
    "\t# Calculate root mean square (RMS)\n",
    "\trms = np.sqrt(np.mean(np.square(data), axis=1))\n",
    "\t\n",
    "\t# Calculate Shannon entropy\n",
    "\tdef shannon_entropy(signal):\n",
    "\t\t# Normalize the signal\n",
    "\t\ts = np.sum(signal)\n",
    "\t\t\n",
    "\t\tprob_density = signal / (1 if s is None else s)\n",
    "\t\t# Use scipy's entropy function to calculate Shannon entropy\n",
    "\t\treturn entropy(prob_density, base=2)\n",
    "\t\n",
    "\tshannon_entropy_values = np.apply_along_axis(shannon_entropy, 1, data)\n",
    "\t\n",
    "\t# Calculate mean and standard deviation of the first derivative\n",
    "\tfirst_derivative = np.diff(data, axis=1)\n",
    "\tmean_first_derivative = np.mean(first_derivative, axis=1)\n",
    "\tstd_dev_first_derivative = np.std(first_derivative, axis=1)\n",
    "\t\n",
    "\t# Return a dictionary with all results\n",
    "\treturn {\n",
    "\t\t'mean': mean,\n",
    "\t\t'median': median,\n",
    "\t\t'standard_deviation': std_dev,\n",
    "\t\t'variance': variance,\n",
    "\t\t'interquartile_range': iqr,\n",
    "\t\t'skewness': skewness,\n",
    "\t\t'kurtosis': kurtosis,\n",
    "\t\t'root_mean_square': rms,\n",
    "\t\t'shannon_entropy': shannon_entropy_values,\n",
    "\t\t'mean_first_derivative': mean_first_derivative,\n",
    "\t\t'std_dev_first_derivative': std_dev_first_derivative\n",
    "\t}\n",
    "\n",
    "def compute_freq_domain_statistics(X, sample_rate = 125):\n",
    "\tfeatures = {\n",
    "\t\t'first_moment': [],\n",
    "\t\t'second_moment': [],\n",
    "\t\t'third_moment': [],\n",
    "\t\t'fourth_moment': [],\n",
    "\t\t'median_frequency': [],\n",
    "\t\t'spectral_entropy': [],\n",
    "\t\t'total_spectral_power': [],\n",
    "\t\t'peak_amplitude': []\n",
    "\t}\n",
    "\t\n",
    "\tfor signal in X:\n",
    "\t\t# Compute the FFT\n",
    "\t\tfreq_data = fft(signal)\n",
    "\t\t# Get the power spectrum\n",
    "\t\tpower_spectrum = np.abs(freq_data)**2\n",
    "\t\t# Get the frequencies for bins\n",
    "\t\tfreqs = np.fft.fftfreq(len(signal), 1/sample_rate)\n",
    "\t\t\n",
    "\t\t# Consider only positive frequencies\n",
    "\t\tpos_mask = freqs > 0\n",
    "\t\tfreqs = freqs[pos_mask]\n",
    "\t\tpower_spectrum = power_spectrum[pos_mask]\n",
    "\t\t\n",
    "\t\t# Moments of the power spectrum\n",
    "\t\ttotal_power = np.sum(power_spectrum)\n",
    "\t\taverage_power = power_spectrum / total_power\n",
    "\t\tfeatures['first_moment'].append(np.sum(freqs * average_power))\n",
    "\t\tfeatures['second_moment'].append(np.sum((freqs**2) * average_power))\n",
    "\t\tfeatures['third_moment'].append(np.sum((freqs**3) * average_power))\n",
    "\t\tfeatures['fourth_moment'].append(np.sum((freqs**4) * average_power))\n",
    "\t\t\n",
    "\t\t# Median frequency\n",
    "\t\tcumulative_power = np.cumsum(average_power)\n",
    "\t\tmedian_freq_index = np.where(cumulative_power >= 0.5)[0][0]\n",
    "\t\tfeatures['median_frequency'].append(freqs[median_freq_index])\n",
    "\t\t\n",
    "\t\t# Spectral entropy\n",
    "\t\tfeatures['spectral_entropy'].append(entropy(average_power))\n",
    "\t\t\n",
    "\t\t# Total spectral power\n",
    "\t\tfeatures['total_spectral_power'].append(total_power)\n",
    "\t\t\n",
    "\t\t# Peak amplitude in 0 to 10 Hz\n",
    "\t\trelevant_mask = (freqs >= 0) & (freqs <= 10)\n",
    "\t\tpeak_amplitude = np.max(power_spectrum[relevant_mask]) if np.any(relevant_mask) else 0\n",
    "\t\tfeatures['peak_amplitude'].append(peak_amplitude)\n",
    "\t\n",
    "\t# Convert lists to numpy arrays for easier handling later\n",
    "\tfor key in features:\n",
    "\t\tfeatures[key] = np.array(features[key])\n",
    "\n",
    "\treturn features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beat to Beat Analysis\n",
    "def detect_systolic_peaks(abp_signal, sampling_rate):\n",
    "\t# Use a peak detection algorithm, scipy's find_peaks could be suitable\n",
    "\tpeaks, _ = signal.find_peaks(abp_signal, distance=sampling_rate/2)  # distance based on typical human heart rate (0.5 sec apart)\n",
    "\treturn peaks\n",
    "\n",
    "\n",
    "def compute_intervals(peaks, sampling_rate):\n",
    "\t# Convert peak indices to time intervals in seconds\n",
    "\tintervals = np.diff(peaks) / sampling_rate\n",
    "\treturn intervals\n",
    "\n",
    "\n",
    "def compute_B2BInterval(data, sampling_rate=125):\n",
    "\tresults = []\n",
    "\tfor signal in data:\n",
    "\t\tpeaks = detect_systolic_peaks(signal, sampling_rate)\n",
    "\t\tintervals = compute_intervals(peaks, sampling_rate)\n",
    "\t\tresults.append(intervals)\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def calculate_poincare_features(intervals):\n",
    "    \"\"\"\n",
    "    Calculate the PoincarÃ© plot features SD1 and SD2 for a given set of intervals.\n",
    "    \n",
    "    Args:\n",
    "    intervals (np.array): numpy array of beat-to-beat intervals\n",
    "    \n",
    "    Returns:\n",
    "    dict: dictionary with 'SD1', 'SD2', and 'SD1_SD2_ratio'\n",
    "    \"\"\"\n",
    "    if len(intervals) < 2:\n",
    "        return {'SD1': np.nan, 'SD2': np.nan, 'SD1_SD2_ratio': np.nan}\n",
    "\n",
    "    # Calculate differences between consecutive intervals\n",
    "    diff_intervals = np.diff(intervals)\n",
    "    \n",
    "    # Compute SD1 and SD2\n",
    "    SD1 = np.sqrt(np.var(diff_intervals, ddof=1) / 2)\n",
    "    SD2 = np.sqrt(2 * np.var(intervals, ddof=1) - (np.var(diff_intervals, ddof=1) / 2))\n",
    "    \n",
    "    # Compute the SD1/SD2 ratio\n",
    "    SD1_SD2_ratio = SD1 / SD2\n",
    "    \n",
    "    return {'SD1': SD1, 'SD2': SD2, 'SD1_SD2_ratio': SD1_SD2_ratio}\n",
    "\n",
    "\n",
    "def compute_poincare_features(interval_data):\n",
    "    SD1 = []\n",
    "    SD2 = []\n",
    "    SD1_SD2_ratio = []\n",
    "\n",
    "    for intervals in interval_data:\n",
    "        result = calculate_poincare_features(intervals)\n",
    "        SD1.append(result['SD1'])\n",
    "        SD2.append(result['SD2'])\n",
    "        SD1_SD2_ratio.append(result['SD1_SD2_ratio'])\n",
    "    \n",
    "    return {\n",
    "        'SD1': np.array(SD1),\n",
    "        'SD2': np.array(SD2),\n",
    "        'SD1_SD2_ratio': np.array(SD1_SD2_ratio)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3633484/453419504.py:41: RuntimeWarning: invalid value encountered in sqrt\n",
      "  SD2 = np.sqrt(2 * np.var(intervals, ddof=1) - (np.var(diff_intervals, ddof=1) / 2))\n"
     ]
    }
   ],
   "source": [
    "train_time_domain_stats = compute_time_domain_statistics(X_train_filtered)\n",
    "train_time_domain_stats_arr = np.column_stack(list(train_time_domain_stats.values()))\n",
    "\n",
    "train_freq_domain_stats = compute_freq_domain_statistics(X_train_filtered)\n",
    "train_freq_domain_stats_arr = np.column_stack(list(train_freq_domain_stats.values()))\n",
    "\n",
    "X_train_B2Bintervals = compute_B2BInterval(X_train_filtered)\n",
    "X_train_poincare_features = compute_poincare_features(X_train_B2Bintervals)\n",
    "X_train_poincare_features_arr = np.column_stack(list(X_train_poincare_features.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_domain_stats = compute_time_domain_statistics(X_test)\n",
    "test_time_domain_stats_arr = np.column_stack(list(test_time_domain_stats.values()))\n",
    "\n",
    "test_freq_domain_stats = compute_freq_domain_statistics(X_test)\n",
    "test_freq_domain_stats_arr = np.column_stack(list(test_freq_domain_stats.values()))\n",
    "\n",
    "X_test_B2Bintervals = compute_B2BInterval(X_test)\n",
    "X_test_poincare_features = compute_poincare_features(X_test_B2Bintervals)\n",
    "X_test_poincare_features_arr = np.column_stack(list(X_test_poincare_features.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17159, 11) (17159, 8) (17159, 3)\n",
      "(8207, 11) (8207, 8) (8207, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_time_domain_stats_arr.shape, train_freq_domain_stats_arr.shape, X_train_poincare_features_arr.shape)\n",
    "\n",
    "print(test_time_domain_stats_arr.shape, test_freq_domain_stats_arr.shape, X_test_poincare_features_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined array: (17159, 22)\n",
      "Shape of the combined array: (8207, 22)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.hstack((train_time_domain_stats_arr, train_freq_domain_stats_arr, X_train_poincare_features_arr))\n",
    "\n",
    "train_features_clean = np.nan_to_num(train_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Check the shape of the resulting array to ensure it is correct\n",
    "print(\"Shape of the combined array:\", train_features_clean.shape)\n",
    "\n",
    "test_features = np.hstack((test_time_domain_stats_arr, test_freq_domain_stats_arr, X_test_poincare_features_arr))\n",
    "test_features_clean = np.nan_to_num(test_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "print(\"Shape of the combined array:\", test_features_clean.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (should be approx. 0): [-2.96832447e-15 -1.35395577e-16  1.61230148e-15 -1.61237265e-15\n",
      "  9.77058052e-17 -3.59724812e-15 -8.35781056e-17  9.07136130e-16\n",
      " -4.99427607e-15 -8.12011129e-18 -2.07358688e-14 -1.10992863e-14\n",
      " -1.02606503e-15  1.17130826e-15 -2.09231975e-16 -3.91842926e-15\n",
      " -4.22503625e-15 -5.59198775e-14  1.39890430e-15 -1.00932336e-15\n",
      "  1.03604209e-16 -1.71917718e-15]\n",
      "Standard Deviation (should be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assume train_features is loaded and has shape (597, 22)\n",
    "# Assume y_train_filtered is loaded and has shape (597,)\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(train_features_clean)\n",
    "\n",
    "X_test_scaled = scaler.transform(test_features_clean)\n",
    "\n",
    "# Optional: Verify mean and std deviation\n",
    "print(\"Mean (should be approx. 0):\", np.mean(X_train_scaled, axis=0))\n",
    "print(\"Standard Deviation (should be 1):\", np.std(X_train_scaled, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9513957689842065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.95      8618\n",
      "         1.0       0.98      0.92      0.95      8541\n",
      "\n",
      "    accuracy                           0.95     17159\n",
      "   macro avg       0.95      0.95      0.95     17159\n",
      "weighted avg       0.95      0.95      0.95     17159\n",
      "\n",
      "[[8442  176]\n",
      " [ 658 7883]]\n",
      "Test Accuracy: 0.9504081881320824\n",
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.95      4104\n",
      "         1.0       0.98      0.92      0.95      4103\n",
      "\n",
      "    accuracy                           0.95      8207\n",
      "   macro avg       0.95      0.95      0.95      8207\n",
      "weighted avg       0.95      0.95      0.95      8207\n",
      "\n",
      "[[4010   94]\n",
      " [ 313 3790]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SVC (Support Vector Classifier) with RBF kernel\n",
    "svm_classifier = SVC(kernel='rbf')\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train_scaled, y_train_filtered)\n",
    "\n",
    "# evaluate the classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = svm_classifier.predict(X_train_scaled)\n",
    "\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train_filtered, y_pred))\n",
    "print(classification_report(y_train_filtered, y_pred))\n",
    "print(confusion_matrix(y_train_filtered, y_pred))\n",
    "\n",
    "\n",
    "y_pred_test = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Classification report:\", classification_report(y_test, y_pred_test))\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
