{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (17235, 3750)\n",
      "Train Labels Shape: (17235,)\n",
      "Validation Features Shape: (1915, 3750)\n",
      "Validation Labels Shape: (1915,)\n",
      "Test Features Shape: (8207, 3750)\n",
      "Test Labels Shape: (8207,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to load data and split into features and labels\n",
    "def load_data(filepath):\n",
    "\t# Load the data from CSV file\n",
    "\tdata = np.genfromtxt(filepath, delimiter=',', skip_header=1)  # skip_header is used if your CSV has a header row\n",
    "\t# Separate features and labels\n",
    "\tX = data[:, :-1]  # all rows, all columns except the last\n",
    "\ty = data[:, -1]   # all rows, last column\n",
    "\treturn X, y\n",
    "\n",
    "# Load datasets\n",
    "X_train, y_train = load_data('../data/ABP_train_samples.csv')\n",
    "X_val, y_val = load_data('../data/ABP_val_samples.csv')\n",
    "X_test, y_test = load_data('../data/ABP_test_samples.csv')\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Train Features Shape:\", X_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Validation Features Shape:\", X_val.shape)\n",
    "print(\"Validation Labels Shape:\", y_val.shape)\n",
    "print(\"Test Features Shape:\", X_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Removing values from X_train that are less than or equal to 0 for more than 50% of duration\n",
    "mask = X_train <= 0\n",
    "count_le_zero = np.sum(mask, axis=1)\n",
    "# Find the rows where more than 50% of the elements are <= 0\n",
    "rows_to_remove = count_le_zero > (X_train.shape[1] / 2)\n",
    "rows_to_keep = ~rows_to_remove\n",
    "print(rows_to_keep)\n",
    "\n",
    "# Apply the mask to filter out the rows in X_train\n",
    "X_train_filtered = X_train[rows_to_keep]\n",
    "# Assuming y_train is a corresponding array of labels\n",
    "y_train_filtered = y_train[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import entropy\n",
    "import scipy.signal as signal\n",
    "import nolds\n",
    "\n",
    "def compute_time_domain_statistics(data):\n",
    "\t# Calculate mean\n",
    "\tmean = np.mean(data, axis=1)\n",
    "\t\n",
    "\t# Calculate median\n",
    "\tmedian = np.median(data, axis=1)\n",
    "\t\n",
    "\t# Calculate standard deviation\n",
    "\tstd_dev = np.std(data, axis=1)\n",
    "\t\n",
    "\t# Calculate variance\n",
    "\tvariance = np.var(data, axis=1)\n",
    "\t\n",
    "\t# Calculate interquartile range\n",
    "\tiqr = stats.iqr(data, axis=1)\n",
    "\t\n",
    "\t# Calculate skewness\n",
    "\tskewness = stats.skew(data, axis=1)\n",
    "\t\n",
    "\t# Calculate kurtosis\n",
    "\tscaled_data = data * 1e6  # Scale up the data to avoid precision issues\n",
    "\tkurtosis = stats.kurtosis(scaled_data, axis=1)\n",
    "\t\n",
    "\t# Calculate root mean square (RMS)\n",
    "\trms = np.sqrt(np.mean(np.square(data), axis=1))\n",
    "\t\n",
    "\t# Calculate Shannon entropy\n",
    "\tdef shannon_entropy(signal):\n",
    "\t\t# Normalize the signal\n",
    "\t\ts = np.sum(signal)\n",
    "\t\t\n",
    "\t\tprob_density = signal / (1 if s is None else s)\n",
    "\t\t# Use scipy's entropy function to calculate Shannon entropy\n",
    "\t\treturn entropy(prob_density, base=2)\n",
    "\t\n",
    "\tshannon_entropy_values = np.apply_along_axis(shannon_entropy, 1, data)\n",
    "\t\n",
    "\t# Calculate mean and standard deviation of the first derivative\n",
    "\tfirst_derivative = np.diff(data, axis=1)\n",
    "\tmean_first_derivative = np.mean(first_derivative, axis=1)\n",
    "\tstd_dev_first_derivative = np.std(first_derivative, axis=1)\n",
    "\t\n",
    "\t# Return a dictionary with all results\n",
    "\treturn {\n",
    "\t\t'mean': mean,\n",
    "\t\t'median': median,\n",
    "\t\t'standard_deviation': std_dev,\n",
    "\t\t'variance': variance,\n",
    "\t\t'interquartile_range': iqr,\n",
    "\t\t'skewness': skewness,\n",
    "\t\t'kurtosis': kurtosis,\n",
    "\t\t'root_mean_square': rms,\n",
    "\t\t'shannon_entropy': shannon_entropy_values,\n",
    "\t\t'mean_first_derivative': mean_first_derivative,\n",
    "\t\t'std_dev_first_derivative': std_dev_first_derivative\n",
    "\t}\n",
    "\n",
    "def compute_freq_domain_statistics(X, sample_rate = 125):\n",
    "\tfeatures = {\n",
    "\t\t'first_moment': [],\n",
    "\t\t'second_moment': [],\n",
    "\t\t'third_moment': [],\n",
    "\t\t'fourth_moment': [],\n",
    "\t\t'median_frequency': [],\n",
    "\t\t'spectral_entropy': [],\n",
    "\t\t'total_spectral_power': [],\n",
    "\t\t'peak_amplitude': []\n",
    "\t}\n",
    "\t\n",
    "\tfor signal in X:\n",
    "\t\t# Compute the FFT\n",
    "\t\tfreq_data = fft(signal)\n",
    "\t\t# Get the power spectrum\n",
    "\t\tpower_spectrum = np.abs(freq_data)**2\n",
    "\t\t# Get the frequencies for bins\n",
    "\t\tfreqs = np.fft.fftfreq(len(signal), 1/sample_rate)\n",
    "\t\t\n",
    "\t\t# Consider only positive frequencies\n",
    "\t\tpos_mask = freqs > 0\n",
    "\t\tfreqs = freqs[pos_mask]\n",
    "\t\tpower_spectrum = power_spectrum[pos_mask]\n",
    "\t\t\n",
    "\t\t# Moments of the power spectrum\n",
    "\t\ttotal_power = np.sum(power_spectrum)\n",
    "\t\taverage_power = power_spectrum / total_power\n",
    "\t\tfeatures['first_moment'].append(np.sum(freqs * average_power))\n",
    "\t\tfeatures['second_moment'].append(np.sum((freqs**2) * average_power))\n",
    "\t\tfeatures['third_moment'].append(np.sum((freqs**3) * average_power))\n",
    "\t\tfeatures['fourth_moment'].append(np.sum((freqs**4) * average_power))\n",
    "\t\t\n",
    "\t\t# Median frequency\n",
    "\t\tcumulative_power = np.cumsum(average_power)\n",
    "\t\tmedian_freq_index = np.where(cumulative_power >= 0.5)[0][0]\n",
    "\t\tfeatures['median_frequency'].append(freqs[median_freq_index])\n",
    "\t\t\n",
    "\t\t# Spectral entropy\n",
    "\t\tfeatures['spectral_entropy'].append(entropy(average_power))\n",
    "\t\t\n",
    "\t\t# Total spectral power\n",
    "\t\tfeatures['total_spectral_power'].append(total_power)\n",
    "\t\t\n",
    "\t\t# Peak amplitude in 0 to 10 Hz\n",
    "\t\trelevant_mask = (freqs >= 0) & (freqs <= 10)\n",
    "\t\tpeak_amplitude = np.max(power_spectrum[relevant_mask]) if np.any(relevant_mask) else 0\n",
    "\t\tfeatures['peak_amplitude'].append(peak_amplitude)\n",
    "\t\n",
    "\t# Convert lists to numpy arrays for easier handling later\n",
    "\tfor key in features:\n",
    "\t\tfeatures[key] = np.array(features[key])\n",
    "\n",
    "\treturn features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beat to Beat Analysis\n",
    "def detect_systolic_peaks(signal, sampling_rate):\n",
    "\t# Use a peak detection algorithm, scipy's find_peaks could be suitable\n",
    "\tpeaks, _ = signal.find_peaks(signal, distance=sampling_rate/2)  # distance based on typical human heart rate (0.5 sec apart)\n",
    "\treturn peaks\n",
    "\n",
    "\n",
    "def compute_intervals(peaks, sampling_rate):\n",
    "\t# Convert peak indices to time intervals in seconds\n",
    "\tintervals = np.diff(peaks) / sampling_rate\n",
    "\treturn intervals\n",
    "\n",
    "\n",
    "def compute_B2BInterval(data, sampling_rate=125):\n",
    "\tresults = []\n",
    "\tfor signal in data:\n",
    "\t\tpeaks = detect_systolic_peaks(signal, sampling_rate)\n",
    "\t\tintervals = compute_intervals(peaks, sampling_rate)\n",
    "\t\tresults.append(intervals)\n",
    "\treturn results\n",
    "\n",
    "\n",
    "def calculate_poincare_features(intervals):\n",
    "\t\"\"\"\n",
    "\tCalculate the PoincarÃ© plot features SD1 and SD2 for a given set of intervals.\n",
    "\t\n",
    "\tArgs:\n",
    "\tintervals (np.array): numpy array of beat-to-beat intervals\n",
    "\t\n",
    "\tReturns:\n",
    "\tdict: dictionary with 'SD1', 'SD2', and 'SD1_SD2_ratio'\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tif len(intervals) < 2:\n",
    "\t\treturn {'SD1': np.nan, 'SD2': np.nan, 'SD1_SD2_ratio': np.nan}\n",
    "\n",
    "\t# Calculate differences between consecutive intervals\n",
    "\tdiff_intervals = np.diff(intervals)\n",
    "\t\n",
    "\t# Compute SD1 and SD2\n",
    "\tSD1 = np.sqrt(np.var(diff_intervals, ddof=1) / 2)\n",
    "\tSD2 = np.sqrt(2 * np.var(intervals, ddof=1) - (np.var(diff_intervals, ddof=1) / 2))\n",
    "\t\n",
    "\t# Compute the SD1/SD2 ratio\n",
    "\tSD1_SD2_ratio = SD1 / SD2\n",
    "\t\n",
    "\treturn {'SD1': SD1, 'SD2': SD2, 'SD1_SD2_ratio': SD1_SD2_ratio}\n",
    "\n",
    "\n",
    "def compute_poincare_features(interval_data):\n",
    "\tSD1 = []\n",
    "\tSD2 = []\n",
    "\tSD1_SD2_ratio = []\n",
    "\n",
    "\tfor intervals in interval_data:\n",
    "\t\tresult = calculate_poincare_features(intervals)\n",
    "\t\tSD1.append(result['SD1'])\n",
    "\t\tSD2.append(result['SD2'])\n",
    "\t\tSD1_SD2_ratio.append(result['SD1_SD2_ratio'])\n",
    "\t\n",
    "\treturn {\n",
    "\t\t'SD1': np.array(SD1),\n",
    "\t\t'SD2': np.array(SD2),\n",
    "\t\t'SD1_SD2_ratio': np.array(SD1_SD2_ratio)\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beat to Beat Difference Features\n",
    "\n",
    "def detect_peaks(data_signal, sampling_rate=125):\n",
    "    # Calculate the minimum distance between peaks based on a typical heart rate\n",
    "    # Example: a heart rate range from 60 to 100 beats per minute\n",
    "    min_distance = int(sampling_rate * 0.5)  # 0.5 seconds to avoid too frequent peaks\n",
    "    peaks, _ = signal.find_peaks(data_signal, distance=min_distance)\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def extract_b2b_segments(data_signal):\n",
    "    # Extract segments of the ABP signal between consecutive peaks\n",
    "    segments = []\n",
    "    peaks = detect_peaks(data_signal, sampling_rate=125)\n",
    "    for i in range(1, len(peaks)):\n",
    "        start_idx = peaks[i-1]\n",
    "        end_idx = peaks[i]\n",
    "        segment = data_signal[start_idx:end_idx]\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "def calculate_b2b_diff_features(segments):\n",
    "\tmean, median, stdev, variance, iqr, range, skewness, kurtosis, rms, sample_entropy \\\n",
    "\t, shanon_entropy, mean_first_der, stdev_first_der = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "\tfor s in segments:\n",
    "\t\tmean.append(np.mean(s))\n",
    "\t\tmedian.append(np.median(s))\n",
    "\t\tstdev.append(np.std(s))\n",
    "\t\tvariance.append(np.var(s))\n",
    "\t\trange.append(np.max(s) - np.min(s))\n",
    "\t\tiqr.append(stats.iqr(s))\n",
    "\t\tskewness.append(stats.skew(s))\n",
    "\t\tscaled_data = s * 1e6  # Scale up the data to avoid precision issues\n",
    "\t\tkurtosis.append(stats.kurtosis(scaled_data))\n",
    "\t\t\n",
    "\t\t# Calculate root mean square (RMS)\n",
    "\t\trms.append(np.sqrt(np.mean(np.square(s))))\n",
    "\t\t\n",
    "\n",
    "\t\tm=2\n",
    "\t\tr=0.2 * np.std(s)\n",
    "\t\tsample_entropy.append(nolds.sampen(s, m, r))\n",
    "\t\t\n",
    "\t\t# Calculate Shannon entropy\n",
    "\t\tdef shannon_entropy(signal):\n",
    "\t\t\t# Normalize the signal\n",
    "\t\t\ts = np.sum(signal)\n",
    "\t\t\t\n",
    "\t\t\tprob_density = signal / (1 if s is None else s)\n",
    "\t\t\t# Use scipy's entropy function to calculate Shannon entropy\n",
    "\t\t\treturn entropy(prob_density, base=2)\n",
    "\t\t\n",
    "\t\tshanon_entropy.append(shannon_entropy(s))\n",
    "\t\t\n",
    "\t\t# Calculate mean and standard deviation of the first derivative\n",
    "\t\tfirst_derivative = np.diff(s)\n",
    "\t\tmean_first_der.append(np.mean(first_derivative))\n",
    "\t\tstdev_first_der.append(np.std(first_derivative))\n",
    "\t\n",
    "\treturn {\n",
    "\t\t 'IQR_mean': stats.iqr(mean)\n",
    "\t\t,'IQR_median': stats.iqr(median)\n",
    "\t\t,'IQR_stdev': stats.iqr(stdev)\n",
    "\t\t,'IQR_variance': stats.iqr(variance)\n",
    "\t\t,'IQR_iqr': stats.iqr(iqr)\n",
    "\t\t,'IQR_range': stats.iqr(range)\n",
    "\t\t,'IQR_skewness': stats.iqr(skewness)\n",
    "\t\t,'IQR_kurtosis': stats.iqr(kurtosis)\n",
    "\t\t,'IQR_rms': stats.iqr(rms)\n",
    "\t\t,'IQR_sample_entropy': stats.iqr(sample_entropy)\n",
    "\t\t,'IQR_Shanon_entropy': stats.iqr(shanon_entropy)\n",
    "\t\t,'IQR_mean_first_der': stats.iqr(mean_first_der)\n",
    "\t\t,'IQR_stdev_first_der': stats.iqr(stdev_first_der)\n",
    "\t}\n",
    "\n",
    "\n",
    "def compute_b2b_diff_features(data):\n",
    "\tIQR_mean, IQR_median, IQR_stdev, IQR_variance, IQR_iqr, IQR_range, IQR_skewness = [],[],[],[],[],[],[]\n",
    "\tIQR_kurtosis, IQR_rms, IQR_sample_entropy, IQR_Shanon_entropy, IQR_mean_first_der, IQR_stdev_first_der = [],[],[],[],[],[]\n",
    "\t\n",
    "\t# Get the beat to beat segments. For each epoch, extract the segments\n",
    "\tb2b_segments_list=[]\n",
    "\tfor ep in data:\n",
    "\t\tb2b_segments_list.append(extract_b2b_segments(ep))\n",
    "\n",
    "\t# return b2b_segments_list\n",
    "\tfor segments in b2b_segments_list:\n",
    "\t\tresult = calculate_b2b_diff_features(segments)\n",
    "\t\tIQR_mean.append(result['IQR_mean'])\n",
    "\t\tIQR_median.append(result['IQR_median'])\n",
    "\t\tIQR_stdev.append(result['IQR_stdev'])\n",
    "\t\tIQR_variance.append(result['IQR_variance'])\n",
    "\t\tIQR_iqr.append(result['IQR_iqr'])\n",
    "\t\tIQR_range.append(result['IQR_range'])\n",
    "\t\tIQR_skewness.append(result['IQR_skewness'])\n",
    "\t\tIQR_kurtosis.append(result['IQR_kurtosis'])\n",
    "\t\tIQR_rms.append(result['IQR_rms'])\n",
    "\t\tIQR_sample_entropy.append(result['IQR_sample_entropy'])\n",
    "\t\tIQR_Shanon_entropy.append(result['IQR_Shanon_entropy'])\n",
    "\t\tIQR_mean_first_der.append(result['IQR_mean_first_der'])\n",
    "\t\tIQR_stdev_first_der.append(result['IQR_stdev_first_der'])\n",
    "\t\n",
    "\n",
    "\treturn {\n",
    "\t\t 'IQR_mean': np.array(IQR_mean)\n",
    "\t\t,'IQR_median': np.array(IQR_median)\n",
    "\t\t,'IQR_stdev': np.array(IQR_stdev)\n",
    "\t\t,'IQR_variance': np.array(IQR_variance)\n",
    "\t\t,'IQR_iqr': np.array(IQR_iqr)\n",
    "\t\t,'IQR_range': np.array(IQR_range)\n",
    "\t\t,'IQR_skewness': np.array(IQR_skewness)\n",
    "\t\t,'IQR_kurtosis': np.array(IQR_kurtosis)\n",
    "\t\t,'IQR_rms': np.array(IQR_rms)\n",
    "\t\t,'IQR_sample_entropy': np.array(IQR_sample_entropy)\n",
    "\t\t,'IQR_Shanon_entropy': np.array(IQR_Shanon_entropy)\n",
    "\t\t,'IQR_mean_first_der': np.array(IQR_mean_first_der)\n",
    "\t\t,'IQR_stdev_first_der': np.array(IQR_stdev_first_der)\n",
    "\t}\n",
    "\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b2b_segments\n",
    "# calculate_b2b_diff_features(b2b_segments[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3862946/4263164101.py:42: RuntimeWarning: invalid value encountered in sqrt\n",
      "  SD2 = np.sqrt(2 * np.var(intervals, ddof=1) - (np.var(diff_intervals, ddof=1) / 2))\n",
      "/home/ms5267@drexel.edu/anaconda3/envs/ve-m/lib/python3.10/site-packages/numpy/lib/function_base.py:4657: RuntimeWarning: invalid value encountered in add\n",
      "  lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))\n"
     ]
    }
   ],
   "source": [
    "train_time_domain_stats = compute_time_domain_statistics(X_train_filtered)\n",
    "train_time_domain_stats_arr = np.column_stack(list(train_time_domain_stats.values()))\n",
    "\n",
    "train_freq_domain_stats = compute_freq_domain_statistics(X_train_filtered)\n",
    "train_freq_domain_stats_arr = np.column_stack(list(train_freq_domain_stats.values()))\n",
    "\n",
    "X_train_B2Bintervals = compute_B2BInterval(X_train_filtered)\n",
    "X_train_poincare_features = compute_poincare_features(X_train_B2Bintervals)\n",
    "X_train_poincare_features_arr = np.column_stack(list(X_train_poincare_features.values()))\n",
    "\n",
    "X_train_B2B_diff_features = compute_b2b_diff_features(X_train_filtered)\n",
    "X_train_B2B_diff_features_arr = np.column_stack(list(X_train_B2B_diff_features.values()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_domain_stats = compute_time_domain_statistics(X_test)\n",
    "test_time_domain_stats_arr = np.column_stack(list(test_time_domain_stats.values()))\n",
    "\n",
    "test_freq_domain_stats = compute_freq_domain_statistics(X_test)\n",
    "test_freq_domain_stats_arr = np.column_stack(list(test_freq_domain_stats.values()))\n",
    "\n",
    "X_test_B2Bintervals = compute_B2BInterval(X_test)\n",
    "X_test_poincare_features = compute_poincare_features(X_test_B2Bintervals)\n",
    "X_test_poincare_features_arr = np.column_stack(list(X_test_poincare_features.values()))\n",
    "\n",
    "X_test_B2B_diff_features = compute_b2b_diff_features(X_test)\n",
    "X_test_B2B_diff_features_arr = np.column_stack(list(X_test_B2B_diff_features.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17159, 11) (17159, 8) (17159, 3)\n",
      "(8207, 11) (8207, 8) (8207, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_time_domain_stats_arr.shape, train_freq_domain_stats_arr.shape, X_train_poincare_features_arr.shape, X_train_B2B_diff_features_arr.shape)\n",
    "\n",
    "print(test_time_domain_stats_arr.shape, test_freq_domain_stats_arr.shape, X_test_poincare_features_arr.shap, X_test_B2B_diff_features_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined array: (17159, 22)\n",
      "Shape of the combined array: (8207, 22)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.hstack((train_time_domain_stats_arr, train_freq_domain_stats_arr, X_train_poincare_features_arr))\n",
    "\n",
    "train_features_clean = np.nan_to_num(train_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Check the shape of the resulting array to ensure it is correct\n",
    "print(\"Shape of the combined array:\", train_features_clean.shape)\n",
    "\n",
    "test_features = np.hstack((test_time_domain_stats_arr, test_freq_domain_stats_arr, X_test_poincare_features_arr))\n",
    "test_features_clean = np.nan_to_num(test_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "print(\"Shape of the combined array:\", test_features_clean.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (should be approx. 0): [-2.96832447e-15 -1.35395577e-16  1.61230148e-15 -1.61237265e-15\n",
      "  9.77058052e-17 -3.59724812e-15 -8.35781056e-17  9.07136130e-16\n",
      " -4.99427607e-15 -8.12011129e-18 -2.07358688e-14 -1.10992863e-14\n",
      " -1.02606503e-15  1.17130826e-15 -2.09231975e-16 -3.91842926e-15\n",
      " -4.22503625e-15 -5.59198775e-14  1.39890430e-15 -1.00932336e-15\n",
      "  1.03604209e-16 -1.71917718e-15]\n",
      "Standard Deviation (should be 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assume train_features is loaded and has shape (597, 22)\n",
    "# Assume y_train_filtered is loaded and has shape (597,)\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(train_features_clean)\n",
    "\n",
    "X_test_scaled = scaler.transform(test_features_clean)\n",
    "\n",
    "# Optional: Verify mean and std deviation\n",
    "print(\"Mean (should be approx. 0):\", np.mean(X_train_scaled, axis=0))\n",
    "print(\"Standard Deviation (should be 1):\", np.std(X_train_scaled, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9513957689842065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.95      8618\n",
      "         1.0       0.98      0.92      0.95      8541\n",
      "\n",
      "    accuracy                           0.95     17159\n",
      "   macro avg       0.95      0.95      0.95     17159\n",
      "weighted avg       0.95      0.95      0.95     17159\n",
      "\n",
      "[[8442  176]\n",
      " [ 658 7883]]\n",
      "Test Accuracy: 0.9504081881320824\n",
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.95      4104\n",
      "         1.0       0.98      0.92      0.95      4103\n",
      "\n",
      "    accuracy                           0.95      8207\n",
      "   macro avg       0.95      0.95      0.95      8207\n",
      "weighted avg       0.95      0.95      0.95      8207\n",
      "\n",
      "[[4010   94]\n",
      " [ 313 3790]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SVC (Support Vector Classifier) with RBF kernel\n",
    "svm_classifier = SVC(kernel='rbf')\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train_scaled, y_train_filtered)\n",
    "\n",
    "# evaluate the classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = svm_classifier.predict(X_train_scaled)\n",
    "\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train_filtered, y_pred))\n",
    "print(classification_report(y_train_filtered, y_pred))\n",
    "print(confusion_matrix(y_train_filtered, y_pred))\n",
    "\n",
    "\n",
    "y_pred_test = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Classification report:\", classification_report(y_test, y_pred_test))\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
