{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Utilities import *\n",
    "from tqdm import tqdm\n",
    "from CustomDataset import CustomHD5Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_hd5(hd5file_path, data_group_name, timestamp_group_name):\n",
    "\t\"\"\"\n",
    "\tThis method reads the hd5 file and returns the data and timestamp group of the file\n",
    "\t\"\"\"\n",
    "\twith h5py.File(hd5file_path, 'r') as file:\n",
    "\t\tdataset = file[data_group_name]\n",
    "\t\ttimestamp = file[timestamp_group_name]\n",
    "\n",
    "\t\treturn dataset[:], timestamp[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "hdf5_file_path = '/home/ms5267@drexel.edu/moberg-precicecap/data/Patient_2021-12-21_04_16.h5'\n",
    "annotation_path = '/home/ms5267@drexel.edu/moberg-precicecap/data/20240207-annotations-export-workspace=precicecap-patient=7-annotation_group=90.csv'\n",
    "annotation_metadata = {\n",
    "\t'modality':'ART'\n",
    "\t,'location':'na'\n",
    "\t,'scale_wrt_hd5':1e3\n",
    "}\n",
    "\n",
    "base_file = '../data/ABP_test_samples_5sec.csv'\n",
    "segment_length_sec = 5\n",
    "sampling_frequency = 125\n",
    "data_group_name='Waveforms/ART_na'\n",
    "timestamp_group_name = 'Waveforms/ART_na_Timestamps'\n",
    "\n",
    "row_count = sum(1 for _ in open(base_file)) - 1\n",
    "new_entries_num = (row_count/2) * 8\n",
    "\n",
    "####################################################################################################\n",
    "df_annotation = pd.read_csv(annotation_path)\n",
    "df_annotation_filtered = df_annotation[(df_annotation['modality']==annotation_metadata['modality']) & (df_annotation['location']==annotation_metadata['location'])]\n",
    "artifacts = df_annotation_filtered[[\"start_time\",\"end_time\"]].to_numpy() * int(annotation_metadata['scale_wrt_hd5'])\n",
    "\n",
    "data, timestamp = read_hd5(hdf5_file_path, data_group_name, timestamp_group_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length = segment_length_sec * sampling_frequency\n",
    "\n",
    "# Randomly get a segment that is of length given as segment_length_sec*sampling_frequency\n",
    "# If has artifact, then append to artifact list else append to non-artifact list\n",
    "\n",
    "# reduced_range = int(len(self.timestamp)/segment_length)\n",
    "\n",
    "# Generate num_positive_samples*2 unique random values from 0 to 58360000 without replacement\n",
    "random_values = np.random.choice(len(timestamp), int(new_entries_num*1.5), replace=False)\n",
    "\n",
    "count_negative, i = 0, 0\n",
    "\n",
    "non_artifact_raw=[]\n",
    "while count_negative<new_entries_num:\n",
    "\tstart_idx = random_values[i]\n",
    "\ttemp_ts = timestamp[start_idx : start_idx+segment_length]\n",
    "\tif not has_artifact(temp_ts, artifacts):\n",
    "\t\ttemp_data = data[start_idx: start_idx+segment_length]\n",
    "\t\tif len(temp_data)==segment_length:\n",
    "\t\t\tnon_artifact_raw.append(data[start_idx: start_idx+segment_length])\n",
    "\t\t\tcount_negative+=1\n",
    "\ti+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(base_file, delimiter=',') \n",
    "non_artifact_raw_np = np.array(non_artifact_raw)\n",
    "\n",
    "non_artifact_labels = np.zeros((non_artifact_raw_np.shape[0], 1))  \n",
    "non_artifact_labeled = np.hstack((non_artifact_raw_np, non_artifact_labels)) \n",
    "\n",
    "combined_data = np.vstack((data, non_artifact_labeled))\n",
    "\n",
    "\n",
    "dir_name, file_name = os.path.split(base_file)\n",
    "base, ext = os.path.splitext(file_name)\n",
    "new_file_name = f\"{base}_imbalanced{ext}\"\n",
    "\n",
    "# Combine it back to form the full path\n",
    "imbalanced_file = os.path.join(dir_name, new_file_name)\n",
    "\n",
    "np.savetxt(imbalanced_file, combined_data, delimiter=',')  # You can specify fmt='%f' if you need formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7364, 626), (29452, 625), (36816, 626))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, non_artifact_raw_np.shape, combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
